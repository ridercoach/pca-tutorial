---
title: "PCA and Supervised Learning: A Simple Example"
author: "Eric N. Moyer"
date: "`r Sys.Date()`"
output:
  tufte::tufte_html: default
  tufte::tufte_handout:
    citation_package: natbib
    latex_engine: pdflatex
  tufte::tufte_book:
    citation_package: natbib
    latex_engine: pdflatex
bibliography: skeleton.bib
link-citations: yes
---

```{r setup, include=FALSE}
library(tufte)
library(tidyverse)
library(ggbiplot)
# invalidate cache when the tufte version changes
knitr::opts_chunk$set(tidy = FALSE, cache.extra = packageVersion('tufte'))
options(htmltools.dir.version = FALSE)
source("pca-utilities.R")
```

# Introduction

This exploration is inspired by Luke Hayden's excellent tutorial on DataCamp
about doing Principal Component Analysis in R,^[
[Link](https://www.datacamp.com/community/tutorials/pca-analysis-r) to the
article] in which he uses PCA and the `ggbiplot()` package to visually identify
groups in the data, which might be considered EDA or unsupervised learning.

In this article^[This document was created in RStudio using the **tufte_html** and
**tufte_handout** formats.] I want to use the same data and R function
(`prcomp()`) to investigate how PCA can augment *supervised* learning;
specifically, how the accuracy of a predictive model is affected by:

1. subsetting the original features based on insight gained from PCA;
1. replacing the original features with transformed features.

# The Data

We will use the `mtcars` dataset built into R, but we will drop 
the two categorical features (eighth and ninth columns):

```{r data}
data <- mtcars[c(1:7, 10, 11)]
knitr::kable(
  data[1:4,], caption = 'The first few rows of the data'
)
```

This data set is very small, which is convenient for this article, 
but makes it impractical to split out a test set for model evaluation, so 
we will just use the RSE returned by `lm()` as a yardstick. I don't think 
this will completely invalidate the results, but a good next step would 
be to try what we do here with a bigger set of data and proper model 
validation.

# Model for Evaluating the Effects of PCA

We will use a linear model, as implemented by the R function `lm()`, to 
predict **mpg** from the other features.

PCA requires the data it operates on to be centered and scaled, so, to be 
consistent, we will do the same thing to the non-target features when 
creating our baseline model:

```{r data_baseline}
data_baseline <- as.data.frame(cbind(data[,1], 
    scale(data[,2:9], center = TRUE, scale = TRUE)))
colnames(data_baseline)[1] <- "mpg"
knitr::kable(
  format(data_baseline[1:4,], digits = 3), 
  caption = 'Sample of data for baseline model'
)
```

Our baseline RSE, using all of the original features (which, with only nine
features, is what we would naturally do if we had no insight to the contrary)
is:

```{r mod_baseline}
sigma(lm(mpg ~ ., data_baseline))
```

# Insight vs Prediction

How we use PCA with supervised learning depends on whether we primarily want 
*insight* (by determining which features are most important,) or, 
instead, to make *predictions* as accurately as possible (by
transforming the original features into composite features that better represent
the variation in the data.)

In either case, the contribution of PCA is to find the directions along which our
cloud of data points varies the most;^[If our data includes *n* features, then
each observation is a point in n-dimensional space, and all the observations
together might be envisioned as a "cloud" of such points.] it knows nothing
about prediction.

If we are pursuing insight, then what we hope to gain from PCA is knowledge of
which (if any) original features most closely align with a direction of
variance.  In this case, it is helpful to include the target
feature in the data that gets transformed by PCA.

On the other hand, if we want to make predictions based on PCA-transformed 
features, then we do not include the target feature in PCA, for a couple of 
reasons, not least of which is that when we use the model in production, we 
won't *have* the target feature to transform.

In this article we try out both approaches.

# Approach 1: Feature Selection via PCA

We pass the entire data set, including the target feature **mpg**, to the 
PCA function `prcomp()`, which returns a data structure containing a lot 
of useful information.  We will look at three aspects of it.

## PCA Summary Information

```{r pca1_summary}
pca1 = prcomp(data, center = TRUE, scale = TRUE)
knitr::kable(
  format(as.data.frame(summary(pca1)$importance), 
         digits = 2), caption = 'PCA Summary'
)
```

We see in the above table that PCA transformed our nine original features into
nine principal components, given in order of decreasing importance.  PC1
represents the direction along which our cloud of data points varies the most;
the SD of the data along this direction is about 2.4, and 63% of the "signal" in
the data is seen along this axis. Notice also that the first two PCs together
account for 86% of the meaningful information in this data set.

## Transformed Observations

Also returned by `prcomp()` are the coordinates of the data points in terms of
the principal components. The table below shows these new coordinates for the
same few rows we have been looking at. *The "identities" of the observations
within the data set have not changed;* they are simply being expressed as values
for PC1, PC2, etc, instead of as values for **mpg**, **cyl**, etc.

```{r pca1_xformed_data}
knitr::kable(
  format(pca1$x[1:4,], digits = 1), 
  caption = 'Sample of transformed data'
)
```

When our nine-dimensional data is expressed in terms of the original physical
features, it is difficult to represent visually; we can choose features for 2-D
or 3-D plots, but we have no way of knowing how much of the story those plots
tell.  But now, knowing that most of the variance in the data is captured in
just two PCs, we *can* make a meaningful 2-D plot (see margin.) This plot is not
necessary for what we are doing here, but it is interesting, and shows how PCA
can reveal groups in the data.

```{r pca1_dataplot, fig.margin = TRUE, fig.cap = "The data in terms of PC1 and PC2", fig.width=3.5, fig.height=3.5, cache=TRUE, message=FALSE}
ggplot(as.data.frame(pca1$x)) +
  geom_point(aes(PC1, PC2), color = "blue") +
  xlim(-4, 4) + ylim(-4, 4)
```

## Transformation Matrix

`prcomp()` also returns a matrix that relates the PCs to the original 
coordinate system:

```{r pca1_matrix}
knitr::kable(
  format(as.data.frame(summary(pca1)$rotation), 
         digits = 3), caption = 'Transformation matrix'
)
```

Each column of the above table shows the components (in terms of the original
physical coordinate system) of a unit vector in the direction of one of the PCs.
(This matrix was used by `prcomp()` to produce the transformed data, and could
be used to transform new observations in the same way;^[If you are unfamiliar
with transforming vectors between coordinate systems, the video series *Essence
of Linear Algebra* on [3blue1brown.com](https://www.3blue1brown.com) is an
excellent resource.] see Appendix A for an example.) The greater the magnitude
of a number in this matrix (regardless of sign,) the greater the alignment
between the corresponding physical feature and principal component.

We can combine the transformation matrix and PCA summary into a "heatmap" (see
margin; in each column, darker colors indicate larger components along the
original feature axes, and each columnn is scaled by the proportion of variance
of the PC) to get a visual sense of the overall importance of the original
features:

```{r pca1_heatmap, fig.margin = TRUE, fig.cap = "Overall importance of original features, according to PCA", fig.width=3.5, fig.height=3.5, cache=TRUE, message=FALSE}
prop_var <- matrix(rep(summary(pca1)$importance[2,], 9), 
                   nrow = 9, byrow = TRUE)
df <- cbind(rownames(pca1$rotation), 
            data.frame(pca1$rotation * prop_var))
colnames(df)[1] <- "feat"
df2 <- gather(df, colnames(df)[2:10], 
              key = "PC", value = "comp")
df2$feat <- factor(df2$feat, 
                   levels=(rev(rownames(pca1$rotation))))
ggplot(df2, aes(x = PC, y = feat)) + 
  geom_tile(aes(fill = comp), colour = "white") +
  guides(fill = "none") +
  scale_fill_gradient2(low = "darkblue", 
                       mid = "white", high = "darkred")
```

# Feature Selection

A plot will make it easier to see alignment between the original features
and the first two principal components:^[The `ggbiplot` package will make a plot
similar to this, but it's more fun to do it ourselves.]

```{r pca1_vecplot, fig.cap = "Original features as vectors in PC1-PC2 plane"}
df <- data.frame("feat" = rownames(pca1$rotation),  
                 "PC1" = pca1$rotation[,1], 
                 "PC2" = pca1$rotation[,2])
ggplot(df) + geom_segment(aes(x = 0, y = 0, 
        xend = PC1, yend = PC2, color = feat)) +
  xlim(-0.5, 0.5) + ylim(-0.75, 0.75) + 
  labs(x = "PC1", y = "PC2")
```

We notice in the above plot that our target feature **mpg** and the **cyl** 
feature are closely aligned with the PC1 axis.  So let's try predicting **mpg** 
from *only* **cyl**:

```{r mod_cyl}
sigma(lm(mpg ~ cyl, data_baseline))
```

This yields an RSE of 3.21, which is worse than the baseline of 2.62, so let's 
try something else.  We see that **hp**, **disp**, and **wt** are also somewhat 
aligned with PC1, and, taken together, it appears that their misalignments might 
balance out, so:

```{r mod_cylplus}
sigma(lm(mpg ~ cyl + hp + disp + wt, data_baseline))
```

This yields a *better* fit than the baseline model that uses all the 
features.

So, while it bears repeating that we are using a very small data set and an
overly-simplistic means of model evaluation, we have shown how PCA can be used
for feature selection.  Part of why this worked so well in this example is that
most of the data variation was captured in the first two PCs. This allowed us to
plot the original features as 2-D vectors without losing much information,
making the important features much easier to see. This will not necessarily 
be the case for every data set.

# Approach 2: Prediction Using PCA-Transformed Features

With this approach we do not include the target feature 
in the PCA; here is the summary of the new PC analysis:

```{r pca2_summary}
pca2 = prcomp(data[c(2:9)], center = TRUE, scale = TRUE)
knitr::kable(
  format(as.data.frame(summary(pca2)$importance), 
         digits = 2), 
  caption = 'PCA Summary, target feature removed'
)
```

The numbers have changed a bit, but the first two principal components still 
represent 86% of the variation in the data.  And since the data returned by 
`prcomp()` includes the transformed features, we can very easily try out a 
model using PC1 and PC2:

```{r mod_pc1pc2}
data_pca2 <- as.data.frame(cbind(data[,1], pca2$x))
colnames(data_pca2)[1] <- "mpg"
sigma(lm(mpg ~ PC1 + PC2, data_pca2))
```

This is better than our baseline model, but not as good as the 
combination of original features **cyl**, **hp**, **disp**, and **wt**. 
Let's try adding one more PC:

```{r mod_pc1pc2pc3}
sigma(lm(mpg ~ PC1 + PC2 + PC3, data_pca2))
```

This is the best we have seen yet.  In general, we can keep adding PCs to the
model and the fit will always get better, because the PCs by definition are
along the directions of variance in the model. This is *not* the case with the
original features --- we may often get a better fit by using more of them (or
sometimes, as shown in the previous section, *fewer* of them,) but it is hard to
know intuitively which combination is best;^[There are various procedures for
feature selection that do not utilize PCA.] that was the whole point of how we
used PCA in the previous section.

# Conclusion

PCA is a powerful tool that can improve the accuracy of our models and often 
provide valuable understanding.

If all we need is a black box that makes predictions as accurately as possible, 
then using transformed features as input to the model is the way to go.

However, if we want *insight* --- if our motivation for modeling is to
understand how to control real-world quantities in order to realize some
practical benefit --- then a possible downside to PCA is that it can be
difficult to understand how the principal components map to the real world.

With the data set used in this article, that problem was not extreme; there 
were only a few original features, they were simple physical quantities, and 
the result that **mpg** could be predicted fairly well from only **cyl**, 
**hp**, **disp** and **wt** probably makes intuitive sense to many people. 
However, if we instead had hundreds of features that were more abstract, this 
might not be the case.

# Appendix A: Data Transformation Illustration

Let's use the model we tried in Approach 2 above, that uses PC1, PC2, and 
PC3 as input features, to make a prediction for an observation that was 
not included in the original data set.

## Preparing the Model

We will have to save the model in a variable (instead of just passing it to
`sigma()` to get the RSE):

```{r mod_pca2}
mod_pca2 <- lm(mpg ~ PC1 + PC2 + PC3, data_pca2)
```

## Validating Our Methodology

Just to make sure we know what we are doing, let's try this with a known observation 
first, before venturing into uncharted territory.  Let's use the data for "Lincoln 
Continental", because for this observation the model gives a prediction close to 
reality.  Here is the raw data:

```{r linc_orig_data}
knitr::kable(
  data["Lincoln Continental",], 
  caption = 'Lincoln Continental, raw data'
)
```

It would be nice to have a function that takes a raw feature vector 
to (along with other required information) and returns the prediction. Below 
is such a function, which we will explain in a moment.

```{r pred_func}
pred_func <- function(model, pca_info, df_features) {
  # 1. raw data must be centered & scaled same as PCA input
  df <- (df_features - pca_info$center) / pca_info$scale
  # 2. feature vector in original coords
  v_orig <- as.vector(df)
  # 3. matrix-vector mult to transform to PCA coords
  v_pca <- t(pca_info$rotation) %*% t(v_orig)
  # 4. build df for input to model
  df_in <- data.frame("PC1" = v_pca[1], "PC2" = v_pca[2], 
                      "PC3" = v_pca[3])
  # 5. return prediction
  predict(model, df_in)
}
```

To use this function for our test case, we pass in all the features 
*except* **mpg**, and we should get back a value close to 10.4. We can 
see below that this is indeed the case, so it looks like our methodology is 
good.

```{r pred_linc}
pred_func(mod_pca2, pca2, data["Lincoln Continental",2:9])
```

## Prediction for New Observation

Now let's make a prediction for an unknown case:

```{r data_wombat}
data_wombat <- data.frame("cyl" = 8, "disp" = 500, "hp" = 700, 
                  "drat" = 2.99, "wt" = 4.275, "qsec" = 11.58, 
                  "gear" = 5, "carb" = 2)
rownames(data_wombat)[1] <- "Super Wombat"
knitr::kable(data_wombat,
  caption = 'Super Wombat, raw data'
)
```

```{r pred_wombat}
pred_func(mod_pca2, pca2, data_wombat)
```

The model predicts 8.5 miles-per-gallon for the high performance Super 
Wombat, which does not seem unreasonable.  However, several feature values 
for this unknown case lie outside the ranges of those features in the data 
that was used to fit the model, so we should be especially cautious in 
utilizing this prediction.

## Explanation of Prediction Function

Here is a brief explanation of the steps in our function `pred_func()`:

1. Recall that PCA operates on data which has been centered and scaled; we 
do this to our raw feature values by using the vectors of means (`pca2$center`) and 
scaling factors (`pca2$scale`) that were used in the PCA associated with fitting 
the model.  These vectors are included in the information returned by 
`prcomp()`, which is one reason that data structure is passed to 
`pred_func()`.
1. The main reason for step 2 is that the matrix-vector multiplication we do 
in step 3 doesn't work with a dataframe, and for convenience we built the function 
to accept a dataframe as input.
1. Here we transform the features from the original coordinate system to the 
PCA coordinate system, for which we need the transformation matrix returned by 
`prcomp()`. Note that the matrix and vector are both transposed by passing them 
to `t()`.^[We are using the conventional format of having the transformation 
matrix on the left and the vector to be transformed on the right.  For this to 
work properly, the vector must be a column vector, and the matrix must have 
the directions of the new axes as *rows*.]
1. The R function `predict()` takes as arguments the model to use, and the input 
data as a dataframe, so here we assemble into a dataframe the PCs we want to use 
as input to the model.
1. Use the model to make the prediction.


