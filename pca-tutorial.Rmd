---
title: "PCA and Supervised Learning: A Simple Example"
author: "Eric N. Moyer"
date: "`r Sys.Date()`"
output:
  tufte::tufte_handout:
    citation_package: natbib
    latex_engine: pdflatex
  tufte::tufte_html: default
  tufte::tufte_book:
    citation_package: natbib
    latex_engine: pdflatex
bibliography: skeleton.bib
link-citations: yes
---

```{r setup, include=FALSE}
library(tufte)
library(tidyverse)
library(ggbiplot)
# invalidate cache when the tufte version changes
knitr::opts_chunk$set(tidy = FALSE, cache.extra = packageVersion('tufte'))
options(htmltools.dir.version = FALSE)
source("pca-utilities.R")
```

# Introduction

This exploration is inspired by Luke Hayden's excellent tutorial on DataCamp
about doing Principal Component Analysis in R,^[
[Link](https://www.datacamp.com/community/tutorials/pca-analysis-r) to the
article] in which he uses PCA and the `ggbiplot()` package to visually identify
groups in the data, which might be considered EDA or unsupervised learning.

In this article I want to use the same data and R function (`prcomp()`) to 
investigate how PCA can augment *supervised* learning; specifically, how the 
accuracy of a predictive model is affected by:

1. subsetting the original features based on insight gained from PCA;
1. replacing the original features with a much smaller set of PCA "features".

# The Data

We will use the `mtcars` dataset built into R, but we will drop 
the two categorical features (eighth and ninth columns):

```{r data}
data <- mtcars[c(1:7, 10, 11)]
knitr::kable(
  data[1:4,], caption = 'The first few rows of the data'
)
```

This data set is very small, so nothing in this article will be an 
example of real-world practice.  In particular, we will not be able to 
evaluate the model against a different set of data; we will just use the 
RSE returned by `lm()` as our yardstick. Hopefully this does not completely 
negate the value of the techniques.

# Model for Evluating Effect of PCA

We will use a linear model, as implemented by the R function `lm()`, to 
predict **mpg** from the other features.

PCA requires the data it operates on to be centered and scaled, so, to be 
consistent, we will do the same thing to the non-target features when 
creating our baseline model:

```{r scaled_X}
scaled_X <- scale(data[,2:9], center = TRUE, scale = TRUE)
data_baseline <- as.data.frame(cbind(data[,1], scaled_X))
colnames(data_baseline)[1] <- "mpg"
knitr::kable(
  format(data_baseline[1:4,], digits = 3), 
  caption = 'Sample of data for baseline model'
)
```

Our baseline RSE, using all of the original features (which is what 
we would naturally do if we had no insight to the contrary) is:

```{r mod_baseline}
sigma(lm(mpg ~ ., data_baseline))
```

# A Subtle Point

There is subtle distinction that comes into play when using PCA to 
augment supervised learning, depending on whether we are trying to find 
the most important subset of our original features, or whether we 
instead want to transform our original features into a smaller set 
of "artificial", or composite, features.

In either case, the contribution of PCA is to find the directions along which our
cloud of data points varies the most;^[If our data includes *n* features, then
each observation is a point in n-dimensional space, and all the observations
together might be envisioned as a "cloud" of such points.] it knows nothing
about which feature we are trying to predict.

If we are trying to subset our original features, then what we hope to gain 
from PCA is to determine which (if any) of the features *and the target* are 
more or less aligned with any of the principal components (directions of greatest 
variance.)  So in this case, we include the target in the data that gets 
transformed by PCA.

On the other hand, if we want to convert our original features into a new 
set of composite features,^[More accurately, transform our observations into 
a different coordinate system, in which the axes align with the principal 
components.] then we *cannot* include the target in the PCA, because we will 
"lose it" (it will get mixed into the principal components along with all the 
other features.)

In this article we will try out both approaches.

# Subsetting the Original Features

We pass the entire data set, including the target feature **mpg**, to the 
PCA function `prcomp()`, which returns a data structure containing a lot 
of interesting information.  We will look at three aspects of it.

## PCA Summary Information

```{r pca_subset_summary}
pca = prcomp(data, center = TRUE, scale = TRUE)
m <- format(summary(pca)$importance, digits = 1)
knitr::kable(
  as.data.frame(m), caption = 'PCA Summary.'
)
```

The above table shows nine principal components, because we passed in a data set
with nine features.  The PCs are given in order of decreasing importance.  PC1
represents the direction along which our cloud of data points varies the most;
the SD (row 1 in the table) of the data along this direction is about 2.4, and
63% of the "signal" in the data (row 2 in the table) is seen along this axis. 
Notice also that the first two PCs together account for 86% of the meaningful 
information in this data set (row 3 in the table.)

## Transformed Observations

Another thing returned by `prcomp()` is the coordinates of our data in the new 
system (in which the axes are along the directions of the PCs.)  The table below 
shows these new coordinates for the same few rows we have been looking at. *The 
"identities" of the observations within the data set have not changed;* they are 
simply being expressed as values for PC1, PC2, etc, instead of as values for 
**mpg**, **cyl**, etc.

```{r pca_subset_xformed_data}
knitr::kable(
  format(pca$x[1:4,], digits = 1), 
  caption = 'Sample of transformed data'
)
```

When our data is expressed in terms of the original physical features, 
it is difficult to represent visually; we can choose features for 2-D or 
3-D plots, but we have no way of knowing how close those plots come to telling 
the whole story.  Now, however, knowing that most of the variance in the 
data is captured in just two PCs, we *can* make a meaningful 2-D plot (see margin.) 
This plot is not necessary for what we are doing here, but it is interesting, 
and shows how PCA can reveal groups in the data.

```{r pca_subset_plot_pc1pc2, fig.margin = TRUE, fig.cap = "The data in terms of PC1 and PC2", fig.width=3.5, fig.height=3.5, cache=TRUE, message=FALSE}
xformed_data <- as.data.frame(pca$x)
ggplot(xformed_data) +
  geom_point(aes(PC1, PC2), color = "blue") +
  xlim(-4, 4) + ylim(-4, 4)
```

## Transformation Matrix

The information returned by `prcomp()` also includes a matrix that can be 
used and understood in several different ways (see table below.)

```{r pca_subset_matrix}
m <- as.data.frame(format(pca$rotation, digits = 1))
knitr::kable(
  m, caption = 'Transformation matrix'
)
```

Each column in the table gives the components (in terms of the original 
physical coordinate system) of a unit vector in the direction of one of the 
PCs.  (This matrix was used by `prcomp()` to produce the transformed data, and 
could be used to transform new observations in the same way;^[If you are not 
familiar with this process, there are lots of online sources that will help. If 
you have the time, the 14-part video series *Essence of Linear Algebra* on [3blue1brown](https://www.3blue1brown.com) is excellent.] see Appendix A for 
an example.) The greater the magnitude of a number in this matrix (regardless 
of sign,) the more in alignment are the the corresponding physical feature and 
principal component.

```{r pca_heatmap, fig.margin = TRUE, fig.cap = "Overall importance of original features in the dataset, according to PCA.", fig.width=3.5, fig.height=3.5, cache=TRUE, message=FALSE}
prop_var <- matrix(rep(summary(pca)$importance[2,], 9), 
                   nrow = 9, byrow = TRUE)
df <- cbind(rownames(pca$rotation), 
            data.frame(pca$rotation * prop_var))
colnames(df)[1] <- "feat"
df2 <- gather(df, colnames(df)[2:10], 
              key = "PC", value = "comp")
df2$feat <- factor(df2$feat, 
                   levels=(rev(rownames(pca$rotation))))
ggplot(df2, aes(x = PC, y = feat)) + 
  geom_tile(aes(fill = comp), colour = "white") +
  guides(fill = "none") +
  scale_fill_gradient2(low = "darkblue", 
                       mid = "white", high = "darkred")
```

The heatmap in the margin visually combines the information in the transformation
matrix and the PCA summary table.  In each column in the plot, bolder
colors indicate larger components along the original feature axes, and the
colors in each columnn are scaled by the "Proportion of Variance" in the PCA
Summary table.  So we see again that only the first 2 or 3 PCA
directions are meaningful, and within those we can see which original features
matter more.

# Feature Selection

A plot will make it easier to check for alignment between the original features
and the first two principal components:^[The `ggbiplot` package will make a plot
similar to this, but it's more fun to do it ourselves.]

```{r pca_subset_biplot, fig.cap = "Original features as vectors in PC1-PC2 plane"}
df <- data.frame("feat" = rownames(pca$rotation),  
                 "PC1" = pca$rotation[,1], 
                 "PC2" = pca$rotation[,2])
ggplot(df) + geom_segment(aes(x = 0, y = 0, 
        xend = PC1, yend = PC2, color = feat)) +
  xlim(-0.5, 0.5) + ylim(-0.75, 0.75) + 
  labs(x = "PC1", y = "PC2")
```

The first thing we notice is that our target feature **mpg** and the **cyl** 
feature are closely aligned with the PC1 axis.  So let's try predicting **mpg** 
from *only* **cyl**:

```{r mod_cyl}
sigma(lm(mpg ~ cyl, data_baseline))
```

This yields an RSE of 3.21, which is worse than the baseline of 2.62, so let's 
try something else.  We see that **hp**, **disp**, and **wt** are also somewhat 
aligned with PC1, and, taken together, it appears that their misalignments might 
balance out, so:

```{r mod_cylplus}
sigma(lm(mpg ~ cyl + hp + disp + wt, data_baseline))
```

This yields a *better* fit than the baseline model that uses all the 
features.

So, while it bears repeating that we are using a very small data set, and
therefore an overly-simplistic means of model evaluation, we have shown that PCA
can be used to subset our original features.  Part of why this worked as well as
it did for this particular data set is that most of the data variation was
captured in the first two PCs, allowing us to plot the feature vectors in the
PC1-PC2 plane without losing much information, and this 2-D plot is where the
relationships we leveraged became most apparent.

# Transforming Original Features to PCA Features

Now we will look at another way PCA can augment supervised learning, which 
is by transforming the original features into the coordinate system of the 
principal components, and then using the most meaningful of those transformed 
features as input to the model.

As mentioned previously, with this approach we do not include the target feature 
in the PCA; here is the summary of the new PC analysis:

```{r pca_xfrom_summary}
pca = prcomp(data[c(2:9)], center = TRUE, scale = TRUE)
m <- format(summary(pca)$importance, digits = 1)
knitr::kable(
  as.data.frame(m), 
  caption = 'PCA Summary, target feature removed'
)
```

The numbers have changed a bit, but the first two principal components still 
represent 86% of the variation in the data.  And since the data returned by 
`prcomp()` includes the transformed features, we can very easily try out a 
model using PC1 and PC2:

```{r mod_pc1pc2}
df <- as.data.frame(cbind(data[,1], pca$x))
colnames(df)[1] <- "mpg"
sigma(lm(mpg ~ PC1 + PC2, df))
```

This is better than our baseline model, but actually not as good as the 
combination of original features **cyl**, **hp**, **disp**, and **wt**. 
Let's try adding one more PC:

```{r mod_pc1pc2pc3}
sigma(lm(mpg ~ PC1 + PC2 + PC3, df))
```

This is the best we have seen yet.  In general, we can keep adding PCs in 
order as much as we deem necessary, and the fit will always get better, because 
the PCs by definition are along the directions of variance in the model and the 
analysis returns them in order of decreasing importance (see margin plot.) This 
is *not* the case with the original features --- we may often get a better fit by 
using more of them, but we don't know ahead of time which combination is best; that 
was the whole point of how we used PCA in the previous experiment.^[established 
procedures...]

# Conclusion

So it seems like using the transformed components is a more direct path to getting 
the best possible fit; why not just always do it this way?

The main reason is...

# Apendix A: Data Transformation Illustration

Since we have all the above information at our fingertips, we might 
as well demonstrate how the vector of values for a sample is tranformed 
from one set of axes to another.^[You can learn about this application of 
matrix-vector multiplication from many online sources.] By multiplying the 
first row of centered and scaled values (Mazda RX4) by the rotation matrix 
(transposing as appropriate,) we produce a new vector that matches the first 
row in the transformed data, and by this we confirm that we understand the 
all the information returned by `prcomp()`.

```{r xform_check}
#v_old <- scaled_data[1,]
#A <- t(pca$rotation)
#v_new <- A %*% v_old
#
#knitr::kable(
#  format(t(v_new), digits = 1), 
#  caption = "Transformed data for Mazda RX4"
#)
```

