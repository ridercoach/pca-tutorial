---
title: "PCA and Supervised Learning: A Simple Example"
author: "Eric N. Moyer"
date: "`r Sys.Date()`"
output:
  tufte::tufte_html: default
  tufte::tufte_handout:
    citation_package: natbib
    latex_engine: pdflatex
  tufte::tufte_book:
    citation_package: natbib
    latex_engine: pdflatex
bibliography: skeleton.bib
link-citations: yes
---

```{r setup, include=FALSE}
library(tufte)
library(tidyverse)
library(ggbiplot)
# invalidate cache when the tufte version changes
knitr::opts_chunk$set(tidy = FALSE, cache.extra = packageVersion('tufte'))
options(htmltools.dir.version = FALSE)
source("pca-utilities.R")
```

# Introduction

This exploration is inspired by Luke Hayden's excellent tutorial on DataCamp
about doing Principal Component Analysis in R,^[
[Link](https://www.datacamp.com/community/tutorials/pca-analysis-r) to the
article] in which he uses PCA and the `ggbiplot()` package to visually identify
groups in the data, which might be considered EDA or unsupervised learning.

In this article I want to use the same data and R function (`prcomp()`) to 
investigate how PCA can augment *supervised* learning; specifically, how the 
accuracy of a predictive model is affected by:

1. subsetting the original features based on insight gained from PCA;
1. replacing the original features with transformed features.

# The Data

We will use the `mtcars` dataset built into R, but we will drop 
the two categorical features (eighth and ninth columns):

```{r data}
data <- mtcars[c(1:7, 10, 11)]
knitr::kable(
  data[1:4,], caption = 'The first few rows of the data'
)
```

This data set is very small, which is convenient for this article, 
but makes it impractical to split out a test set for model evaluation, so 
we will just use the RSE returned by `lm()` as a yardstick. I don't think 
this will completely invalidate our results, but a good next step would 
be to try what we do here with a bigger set of data and proper model 
validation.

# Model for Evaluating the Effects of PCA

We will use a linear model, as implemented by the R function `lm()`, to 
predict **mpg** from the other features.

PCA requires the data it operates on to be centered and scaled, so, to be 
consistent, we will do the same thing to the non-target features when 
creating our baseline model:

```{r data_baseline}
data_baseline <- as.data.frame(cbind(data[,1], 
    scale(data[,2:9], center = TRUE, scale = TRUE)))
colnames(data_baseline)[1] <- "mpg"
knitr::kable(
  format(data_baseline[1:4,], digits = 3), 
  caption = 'Sample of data for baseline model'
)
```

Our baseline RSE, using all of the original features (which, with only nine
features, is what we would naturally do if we had no insight to the contrary)
is:

```{r mod_baseline}
sigma(lm(mpg ~ ., data_baseline))
```

# Insight vs Prediction

How we use PCA with supervised learning depends on whether we want to gain
*insight* (by determining which of the original features are most important,) or
whether we simply want to make *predictions* as accurately as possible (by
transforming the original features into composite features that better represent
the variation in the data.)

In either case, the contribution of PCA is to find the directions along which our
cloud of data points varies the most;^[If our data includes *n* features, then
each observation is a point in n-dimensional space, and all the observations
together might be envisioned as a "cloud" of such points.] it knows nothing
about which feature we are trying to predict.

If we are pursuing insight, then what we hope to gain from PCA is knowledge of
which (if any) original features are most closely aligned with a direction of
greatest variance.  In this case, it is probably helpful to include the target
feature in the data that gets transformed by PCA.

On the other hand, if we want to make predictions based on PCA-transformed 
features, then we do not include the target feature in PCA, for a couple of 
reasons, not least of which is that when we use the model in production, we 
won't *have* the target feature to transform.

In this article we will try out both approaches.

# Approach 1: Feature Selection via PCA

We pass the entire data set, including the target feature **mpg**, to the 
PCA function `prcomp()`, which returns a data structure containing a lot 
of interesting information.  We will look at three aspects of it.

## PCA Summary Information

```{r pca1_summary}
pca1 = prcomp(data, center = TRUE, scale = TRUE)
knitr::kable(
  format(as.data.frame(summary(pca1)$importance), 
         digits = 2), caption = 'PCA Summary'
)
```

We see in the above table that PCA transformed our nine original features into
nine principal components, given in order of decreasing importance.  PC1
represents the direction along which our cloud of data points varies the most;
the SD of the data along this direction is about 2.4, and 63% of the "signal" in
the data is seen along this axis. Notice also that the first two PCs together
account for 86% of the meaningful information in this data set.

## Transformed Observations

Also returned by `prcomp()` are the coordinates of the data points in terms of
the principal components. The table below shows these new coordinates for the
same few rows we have been looking at. *The "identities" of the observations
within the data set have not changed;* they are simply being expressed as values
for PC1, PC2, etc, instead of as values for **mpg**, **cyl**, etc.

```{r pca1_xformed_data}
knitr::kable(
  format(pca1$x[1:4,], digits = 1), 
  caption = 'Sample of transformed data'
)
```

When our nine-dimensional data is expressed in terms of the original physical
features, it is difficult to represent visually; we can choose features for 2-D
or 3-D plots, but we have no way of knowing how much of the story those plots
tell.  But now, knowing that most of the variance in the data is captured in
just two PCs, we *can* make a meaningful 2-D plot (see margin.) This plot is not
necessary for what we are doing here, but it is interesting, and shows how PCA
can reveal groups in the data.

```{r pca1_dataplot, fig.margin = TRUE, fig.cap = "The data in terms of PC1 and PC2", fig.width=3.5, fig.height=3.5, cache=TRUE, message=FALSE}
ggplot(as.data.frame(pca1$x)) +
  geom_point(aes(PC1, PC2), color = "blue") +
  xlim(-4, 4) + ylim(-4, 4)
```

## Transformation Matrix

`prcomp()` also returns a matrix that relates the PCs to the original 
coordinate system:

```{r pca1_matrix}
knitr::kable(
  format(as.data.frame(summary(pca1)$rotation), 
         digits = 3), caption = 'Transformation matrix'
)
```

Each column in the table gives the components (in terms of the original 
physical coordinate system) of a unit vector in the direction of one of the 
PCs.  (This matrix was used by `prcomp()` to produce the transformed data, and 
could be used to transform new observations in the same way;^[If you are 
unfamiliar with this process, there are lots of online sources that will help. If 
you have the time, the 14-part video series *Essence of Linear Algebra* on [3blue1brown.com](https://www.3blue1brown.com) is excellent.] see Appendix A for 
an example.) The greater the magnitude of a number in this matrix (regardless 
of sign,) the greater the alignment between the corresponding physical feature and 
principal component.

We can combine the transformation matrix and PCA summary into a "heatmap" (see
margin; in each column, darker colors indicate larger components along the
original feature axes, and each columnn is scaled by the proportion of variance
of the PC) to get a visual sense of the overall importance of the original
features:

```{r pca1_heatmap, fig.margin = TRUE, fig.cap = "Overall importance of original features, according to PCA", fig.width=3.5, fig.height=3.5, cache=TRUE, message=FALSE}
prop_var <- matrix(rep(summary(pca1)$importance[2,], 9), 
                   nrow = 9, byrow = TRUE)
df <- cbind(rownames(pca1$rotation), 
            data.frame(pca1$rotation * prop_var))
colnames(df)[1] <- "feat"
df2 <- gather(df, colnames(df)[2:10], 
              key = "PC", value = "comp")
df2$feat <- factor(df2$feat, 
                   levels=(rev(rownames(pca1$rotation))))
ggplot(df2, aes(x = PC, y = feat)) + 
  geom_tile(aes(fill = comp), colour = "white") +
  guides(fill = "none") +
  scale_fill_gradient2(low = "darkblue", 
                       mid = "white", high = "darkred")
```

# Feature Selection

A plot will make it easier to see alignment between the original features
and the first two principal components:^[The `ggbiplot` package will make a plot
similar to this, but it's more fun to do it ourselves.]

```{r pca1_vecplot, fig.cap = "Original features as vectors in PC1-PC2 plane"}
df <- data.frame("feat" = rownames(pca1$rotation),  
                 "PC1" = pca1$rotation[,1], 
                 "PC2" = pca1$rotation[,2])
ggplot(df) + geom_segment(aes(x = 0, y = 0, 
        xend = PC1, yend = PC2, color = feat)) +
  xlim(-0.5, 0.5) + ylim(-0.75, 0.75) + 
  labs(x = "PC1", y = "PC2")
```

The first thing we notice is that our target feature **mpg** and the **cyl** 
feature are closely aligned with the PC1 axis.  So let's try predicting **mpg** 
from *only* **cyl**:

```{r mod_cyl}
sigma(lm(mpg ~ cyl, data_baseline))
```

This yields an RSE of 3.21, which is worse than the baseline of 2.62, so let's 
try something else.  We see that **hp**, **disp**, and **wt** are also somewhat 
aligned with PC1, and, taken together, it appears that their misalignments might 
balance out, so:

```{r mod_cylplus}
sigma(lm(mpg ~ cyl + hp + disp + wt, data_baseline))
```

This yields a *better* fit than the baseline model that uses all the 
features.

So, while it bears repeating that we are using a very small data set and an
overly-simplistic means of model evaluation, we have shown how PCA can be used
for feature selection.  Part of why this worked so well in this example is that
most of the data variation was captured in the first two PCs. This allowed us to
plot the original features as 2-D vectors without losing much information,
making the important features much easier to see. This will not necessarily always 
be the case.

# Approach 2: Prediction Using PCA-Transformed Features

As mentioned previously, with this approach we do not include the target feature 
in the PCA; here is the summary of the new PC analysis:

```{r pca2_summary}
pca2 = prcomp(data[c(2:9)], center = TRUE, scale = TRUE)
knitr::kable(
  format(as.data.frame(summary(pca2)$importance), 
         digits = 2), 
  caption = 'PCA Summary, target feature removed'
)
```

The numbers have changed a bit, but the first two principal components still 
represent 86% of the variation in the data.  And since the data returned by 
`prcomp()` includes the transformed features, we can very easily try out a 
model using PC1 and PC2:

```{r mod_pc1pc2}
data_pca2 <- as.data.frame(cbind(data[,1], pca2$x))
colnames(data_pca2)[1] <- "mpg"
sigma(lm(mpg ~ PC1 + PC2, data_pca2))
```

This is better than our baseline model, but not as good as the 
combination of original features **cyl**, **hp**, **disp**, and **wt**. 
Let's try adding one more PC:

```{r mod_pc1pc2pc3}
sigma(lm(mpg ~ PC1 + PC2 + PC3, data_pca2))
```

This is the best we have seen yet.  In general, we can keep adding PCs to the
model and the fit will always get better, because the PCs by definition are
along the directions of variance in the model. This is *not* the case with the
original features --- we may often get a better fit by using more of them (or
sometimes, as shown in the previous section, *fewer* of them,) but it is hard to
know intuitively which combination is best;^[There are various procedures for
feature selection that do not utilize PCA.] that was the whole point of how we
used PCA in the previous section.

# Conclusion

PCA is a powerful tool that can improve the accuracy of our models and often 
provide valuable understanding.

If all we need is a black box that makes predictions as accurately as possible, 
then using transformed features as input to the model is the way to go.

However, if we want *insight* --- if our motivation for modeling is to
understand how to control real-world quantities in order to realize some
practical benefit --- then a possible downside to PCA is that it can be
difficult to understand how the principal components map to the real world.

With the data set used in this article, that problem was not extreme; there 
were only a few original features, they were simple physical quantities, and 
the result that **mpg** could be predicted fairly well from only **cyl**, 
**hp**, **disp** and **wt** probably makes intuitive sense to many people. 
However, if we instead had hundreds of features that were more abstract, this 
might not be the case.

# Appendix A: Data Transformation Illustration

Let's use the model we tried in Approach 2 above, that uses PC1, PC2, and 
PC3 as input features, to make a prediction for an observation that was 
not included in the original data set.

## Preparing the Model

We will have to save the model in a variable (instead of just passing it to
`sigma()` to get the RSE):

```{r mod_pca2}
mod_pca2 <- lm(mpg ~ PC1 + PC2 + PC3, data_pca2)
```

## Validating Our Methodology

Just to make sure we know what we are doing, let's try this with a known observation 
first, before venturing into uncharted territory.  Let's use the data for "Lincoln 
Continental", because for this observation the model gives a prediction close to 
reality.  Here is the raw data:

```{r linc_orig_data}
knitr::kable(
  data["Lincoln Continental",], 
  caption = 'Lincoln Continental, raw data'
)
```

It would be nice to have a function that we can pass a raw feature vector 
to, along with whatever else is needed, and get back the prediction. Below 
is such a function, which we will explain in a moment.

```{r pred_func}
pred_func <- function(model, pca_info, df_features) {
  # 1. raw data must be centered & scaled same as PCA input
  df <- (df_features - pca_info$center) / pca_info$scale
  # 2. feature vector in original coords
  v_orig <- as.vector(df)
  # 3. matrix-vector mult to transform to PCA coords
  v_pca <- t(pca_info$rotation) %*% t(v_orig)
  # 4. build df for input to model
  df_in <- data.frame("PC1" = v_pca[1], "PC2" = v_pca[2], 
                      "PC3" = v_pca[3])
  # 5. return prediction
  predict(model, df_in)
}
```

To use this function for our test case, we will pass in all the features 
*except* **mpg**, and we should get back a value close to 10.4. We can 
see below that this is indeed the case, so it looks like our methodology is 
good.

```{r}
pred_func(mod_pca2, pca2, data["Lincoln Continental",2:9])
```

## Prediction for New Observation

Now let's make a prediction for an unknown case:

```{r data_wombat}
data_wombat <- data.frame("cyl" = 8, "disp" = 500, "hp" = 700, 
                  "drat" = 2.99, "wt" = 4.275, "qsec" = 11.58, 
                  "gear" = 5, "carb" = 2)
rownames(data_wombat)[1] <- "Super Wombat"
knitr::kable(data_wombat,
  caption = 'Super Wombat, raw data'
)
```

```{r}
pred_func(mod_pca2, pca2, data_wombat)
```

The model predicts 8.5 miles-per-gallon for the high performance Super 
Wombat, and this is at least somewhat reasonable relative to the  model's 
predictions for other vehicles. However, whenever we feed a model an 
observation that is significantly different than all the data it was trained 
on, we should be extra cautious about the result.

## Explanation of Pre-Processing

dddd


