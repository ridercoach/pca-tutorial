---
title: "PCA and Supervised Learning: A Simple Example"
author: "Eric N. Moyer"
date: "`r Sys.Date()`"
output:
  tufte::tufte_handout:
    citation_package: natbib
    latex_engine: pdflatex
  tufte::tufte_html: default
  tufte::tufte_book:
    citation_package: natbib
    latex_engine: pdflatex
bibliography: skeleton.bib
link-citations: yes
---

```{r setup, include=FALSE}
library(tufte)
library(tidyverse)
library(ggbiplot)
# invalidate cache when the tufte version changes
knitr::opts_chunk$set(tidy = FALSE, cache.extra = packageVersion('tufte'))
options(htmltools.dir.version = FALSE)
source("pca-utilities.R")
```

# Introduction

This exploration is inspired by Luke Hayden's excellent tutorial on DataCamp
about doing Principal Component Analysis in R,^[
[Link](https://www.datacamp.com/community/tutorials/pca-analysis-r) to the
article] in which he uses PCA and the `ggbiplot()` package to visually identify
groups in the data, which might be considered EDA or unsupervised learning.

In this article I want to use the same data and R function (`prcomp()`) to 
investigate how PCA can augment *supervised* learning; specifically, how the 
accuracy of a predictive model is affected by:

1. subsetting the original features based on insight gained from PCA;
1. replacing the original features with a much smaller set of PCA "features".

# The Data

We will use the `mtcars` dataset built into R, but we will drop 
the two categorical features (eighth and ninth columns):

```{r data}
data <- mtcars[c(1:7, 10, 11)]
knitr::kable(
  data[1:4,], caption = 'The first few rows of the data'
)
```

This data set is very small, so nothing in this article will be an 
example of real-world practice.  In particular, we will not be able to 
evaluate the model against a different set of data; we will just use the 
RSE returned by `lm()` as our yardstick. Hopefully this does not completely 
negate the value of the techniques.

# Model for Evluating Effect of PCA

We will use a linear model, as implemented by the R function `lm()`, to 
predict **mpg** from the other features.

PCA requires the data it operates on to be centered and scaled, so, to be 
consistent, we will do the same thing to the non-target features when 
creating our baseline model:

```{r scaled_X}
scaled_X <- scale(data[,2:9], center = TRUE, scale = TRUE)
data_baseline <- as.data.frame(cbind(data[,1], scaled_X))
colnames(data_baseline)[1] <- "mpg"
knitr::kable(
  format(data_baseline[1:4,], digits = 3), 
  caption = 'Sample of data for baseline model'
)
```

Our baseline RSE, using all of the original features (which is what 
we would naturally do if we had no insight to the contrary) is:

```{r mod_baseline}
sigma(lm(mpg ~ ., data_baseline))
```

# A Subtle Point

There is subtle distinction that comes into play when using PCA to 
augment supervised learning, depending on whether we are trying to find 
the most important subset of our original features, or whether we 
instead want to transform our original features into a smaller set 
of "artificial", or composite, features.

In either case, the contribution of PCA is to find the directions along which our
cloud of data points varies the most;^[If our data includes *n* features, then
each observation is a point in n-dimensional space, and all the observations
together might be envisioned as a "cloud" of such points.] it knows nothing
about which feature we are trying to predict.

If we are trying to subset our original features, then what we hope to gain 
from PCA is to determine which (if any) of the features *and the target* are 
more or less aligned with any of the principal components (directions of greatest 
variance.)  So in this case, we include the target in the data that gets 
transformed by PCA.

On the other hand, if we want to convert our original features into a new 
set of composite features,^[More accurately, transform our observations into 
a different coordinate system, in which the axes align with the principal 
components.] then we *cannot* include the target in the PCA, because we will 
"lose it" (it will get mixed into the principal components along with all the 
other features.)

In this article we will try out both approaches.

# Subsetting the Original Features

We pass the entire data set, including the target feature **mpg**, to the 
PCA function `prcomp()`, which returns a data structure containing a lot 
of interesting information.  We will look at three aspects of it.

## PCA Summary Information

```{r pca_subset_summary}
pca = prcomp(data, center = TRUE, scale = TRUE)
m <- format(summary(pca)$importance, digits = 1)
knitr::kable(
  as.data.frame(m), caption = 'PCA Summary.'
)
```

The above table shows nine principal components, because we passed in a data set
with nine features.  The PCs are given in order of decreasing importance.  PC1
represents the direction along which our cloud of data points varies the most;
the SD (row 1 in the table) of the data along this direction is about 2.4, and
63% of the "signal" in the data (row 2 in the table) is seen along this axis. 
Notice also that the first two PCs together account for 86% of the meaningful 
information in this data set (row 3 in the table.)

## Transformed Observations

Another thing returned by `prcomp()` is the coordinates of our data in the new 
system (in which the axes are along the directions of the PCs.)  The table below 
shows these new coordinates for the same few rows we have been looking at. *The 
"identities" of the observations within the data set have not changed;* they are 
simply being expressed as values for PC1, PC2, etc, instead of as values for 
**mpg**, **cyl**, etc.

```{r pca_subset_xformed_data}
knitr::kable(
  format(pca$x[1:4,], digits = 1), 
  caption = 'Sample of transformed data'
)
```

When our data is expressed in terms of the original physical features, 
it is difficult to represent visually; we can choose features for 2-D or 
3-D plots, but we have no way of knowing how close those plots come to telling 
the whole story.  Now, however, knowing that most of the variance in the 
data is captured in just two PCs, we *can* make a meaningful 2-D plot (see margin.) 
This plot is not necessary for what we are doing here, but it is interesting, 
and shows how PCA can reveal groups in the data.

```{r pca_subset_plot_pc1pc2, fig.margin = TRUE, fig.cap = "The data in terms of PC1 and PC2", fig.width=3.5, fig.height=3.5, cache=TRUE, message=FALSE}
xformed_data <- as.data.frame(pca$x)
ggplot(xformed_data) +
  geom_point(aes(PC1, PC2), color = "blue") +
  xlim(-4, 4) + ylim(-4, 4)
```

## Transformation Matrix

The information returned by `prcomp()` also includes a matrix that can be 
used and understood in several different ways (see table below.)

```{r pca_subset_matrix}
m <- as.data.frame(format(pca$rotation, digits = 1))
knitr::kable(
  m, caption = 'Transformation matrix'
)
```

Each column in the table gives the components (in terms of the original 
physical coordinate system) of a unit vector in the direction of one of the 
PCs.  (This matrix was used by `prcomp()` to produce the transformed data, and 
could be used to transform new observations in the same way;^[If you are not 
familiar with this process, there are lots of online sources that will help. If 
you have the time, the 14-part video series *Essence of Linear Algebra* on [3blue1brown](https://www.3blue1brown.com) is excellent.] see Appendix A for 
an example.) The greater the magnitude of a number in this matrix (regardless 
of sign,) the more in alignment are the the corresponding physical feature and 
principal component.

```{r pca_heatmap, fig.margin = TRUE, fig.cap = "Overall importance of original features in the dataset, according to PCA.", fig.width=3.5, fig.height=3.5, cache=TRUE, message=FALSE}
prop_var <- matrix(rep(summary(pca)$importance[2,], 9), 
                   nrow = 9, byrow = TRUE)
df <- cbind(rownames(pca$rotation), 
            data.frame(pca$rotation * prop_var))
colnames(df)[1] <- "feat"
df2 <- gather(df, colnames(df)[2:10], 
              key = "PC", value = "comp")
df2$feat <- factor(df2$feat, 
                   levels=(rev(rownames(pca$rotation))))
ggplot(df2, aes(x = PC, y = feat)) + 
  geom_tile(aes(fill = comp), colour = "white") +
  guides(fill = "none") +
  scale_fill_gradient2(low = "darkblue", 
                       mid = "white", high = "darkred")
```

The heatmap in the margin visually combines the information in the transformation
matrix and the PCA summary table.  In each column in the plot, bolder
colors indicate larger components along the original feature axes, and the
colors in each columnn are scaled by the "Proportion of Variance" in the PCA
Summary table.  So we see again that only the first 2 or 3 PCA
directions are meaningful, and within those we can see which original features
matter more.

# Feature Selection

A plot will make it easier to check for alignment between the original features
and the first two principal components:^[The `ggbiplot` package will make a plot
similar to this, but it's more fun to do it ourselves.]

```{r pca_subset_biplot, fig.cap = "Original features as vectors in PC1-PC2 plane"}
df <- data.frame("feat" = rownames(pca$rotation),  
                 "PC1" = pca$rotation[,1], 
                 "PC2" = pca$rotation[,2])
ggplot(df) + geom_segment(aes(x = 0, y = 0, 
        xend = PC1, yend = PC2, color = feat)) +
  xlim(-0.5, 0.5) + ylim(-0.75, 0.75) + 
  labs(x = "PC1", y = "PC2")
```

The first thing we notice is that our target feature **mpg** and the **cyl** 
feature are closely aligned with the PC1 axis.  So let's try predicting **mpg** 
from *only* **cyl**:

```{r mod_cyl}
sigma(lm(mpg ~ cyl, data_baseline))
```

This yields an RSE of 3.21, which is worse than the baseline of 2.62, so let's 
try something else.  We see that **hp**, **disp**, and **wt** are also somewhat 
aligned with PC1, and, taken together, it appears that their misalignments might 
balance out, so:

```{r mod_cylplus}
sigma(lm(mpg ~ cyl + hp + disp + wt, data_baseline))
```

This yields a *better* fit than the baseline model that uses all the 
features.

So, while it bears repeating that we are using a very small data set, and
therefore an overly-simplistic means of model evaluation, we have shown that PCA
can be used to subset our original features.  Part of why this worked as well as
it did for this particular data set is that most of the data variation was
captured in the first two PCs, allowing us to plot the feature vectors in the
PC1-PC2 plane without losing much information, and this 2-D plot is where the
relationships we leveraged became most apparent.

# Transforming Original Features to PCA Features

Now we will look at another way PCA can augment supervised learning, which 
is by transforming the original features into the coordinate system of the 
principal components, and then using the most meaningful of those transformed 
features as input to the model.

As mentioned previously, with this approach we do not include the target feature 
in the PCA.

# The Data as a Vector Space

Our data has nine features.  For any given "observation" (e.g., Mazda RX4), the
feature values can be thought of as the coordinates of a point (or the
components of a vector, which is essentially the same thing) in nine-dimensional
space, which is described by a coordinate system having nine axes. We have these
axes because they correspond to real-world quantities that we can measure.
However, this real-world coordinate system is not necessarily the best one for
seeing important patterns in the data.

Principal Component Analysis transforms our data onto a new set of
axes^[resources, including 3blue1brown...], as shown below. *The observations 
have not changed*; they are simply being described in terms of a different 
coordinate system, in which the axes are the "principal components" found in 
the data (PC1, PC2, etc) instead of the original measured quantities (mpg, 
cyl, etc.)

The principal components (PCs) are really a new set of features (though not
real-world features) that correspond to what matters most in our data.  Not only
can this provide insight, but it often happens that most of the important
information in the data is described by just a few of the PCs, allowing us to
discard the others without losing much.  Fewer features (especially if you are
starting with hundreds) makes everything downstream a little simpler.

However, there is a downside: it can often be challenging to relate the PCs to
real-world quantities that we can control in order to gain some practical
benefit (which is usually the ultimate goal of any data project.)

# Data Transformation Illustration

Since we have all the above information at our fingertips, we might 
as well demonstrate how the vector of values for a sample is tranformed 
from one set of axes to another.^[You can learn about this application of 
matrix-vector multiplication from many online sources.] By multiplying the 
first row of centered and scaled values (Mazda RX4) by the rotation matrix 
(transposing as appropriate,) we produce a new vector that matches the first 
row in the transformed data, and by this we confirm that we understand the 
all the information returned by `prcomp()`.

```{r xform_check}
#v_old <- scaled_data[1,]
#A <- t(pca$rotation)
#v_new <- A %*% v_old
#
#knitr::kable(
#  format(t(v_new), digits = 1), 
#  caption = "Transformed data for Mazda RX4"
#)
```

# (original document)

The Tufte handout style is a style that Edward Tufte uses in his books and handouts. Tufte's style is known for its extensive use of sidenotes, tight integration of graphics with text, and well-set typography. This style has been implemented in LaTeX and HTML/CSS^[See Github repositories [tufte-latex](https://github.com/tufte-latex/tufte-latex) and [tufte-css](https://github.com/edwardtufte/tufte-css)], respectively. We have ported both implementations into the [**tufte** package](https://github.com/rstudio/tufte). If you want LaTeX/PDF output, you may use the `tufte_handout` format for handouts, and `tufte_book` for books. For HTML output, use `tufte_html`. These formats can be either specified in the YAML metadata at the beginning of an R Markdown document (see an example below), or passed to the `rmarkdown::render()` function. See @R-rmarkdown for more information about **rmarkdown**.

```yaml
---
title: "An Example Using the Tufte Style"
author: "John Smith"
output:
  tufte::tufte_handout: default
  tufte::tufte_html: default
---
```

There are two goals of this package:

1. To produce both PDF and HTML output with similar styles from the same R Markdown document;
1. To provide simple syntax to write elements of the Tufte style such as side notes and margin figures, e.g. when you want a margin figure, all you need to do is the chunk option `fig.margin = TRUE`, and we will take care of the details for you, so you never need to think about `\begin{marginfigure} \end{marginfigure}` or `<span class="marginfigure"> </span>`; the LaTeX and HTML code under the hood may be complicated, but you never need to learn or write such code.

If you have any feature requests or find bugs in **tufte**, please do not hesitate to file them to https://github.com/rstudio/tufte/issues. For general questions, you may ask them on StackOverflow: http://stackoverflow.com/tags/rmarkdown.

# Headings

This style provides first and second-level headings (that is, `#` and `##`), demonstrated in the next section. You may get unexpected output if you try to use `###` and smaller headings.

`r newthought('In his later books')`^[[Beautiful Evidence](http://www.edwardtufte.com/tufte/books_be)], Tufte starts each section with a bit of vertical space, a non-indented paragraph, and sets the first few words of the sentence in small caps. To accomplish this using this style, call the `newthought()` function in **tufte** in an _inline R expression_ `` `r ` `` as demonstrated at the beginning of this paragraph.^[Note you should not assume **tufte** has been attached to your R session. You should either `library(tufte)` in your R Markdown document before you call `newthought()`, or use `tufte::newthought()`.]

# Figures

## Margin Figures

Images and graphics play an integral role in Tufte's work. To place figures in the margin you can use the **knitr** chunk option `fig.margin = TRUE`. For example:

```{r fig-margin, fig.margin = TRUE, fig.cap = "MPG vs horsepower, colored by transmission.", fig.width=3.5, fig.height=3.5, cache=TRUE, message=FALSE}
library(ggplot2)
mtcars2 <- mtcars
mtcars2$am <- factor(
  mtcars$am, labels = c('automatic', 'manual')
)
ggplot(mtcars2, aes(hp, mpg, color = am)) +
  geom_point() + geom_smooth() +
  theme(legend.position = 'bottom')
```

Note the use of the `fig.cap` chunk option to provide a figure caption. You can adjust the proportions of figures using the `fig.width` and `fig.height` chunk options. These are specified in inches, and will be automatically scaled down to fit within the handout margin.

## Arbitrary Margin Content

In fact, you can include anything in the margin using the **knitr** engine named `marginfigure`. Unlike R code chunks ```` ```{r} ````, you write a chunk starting with ```` ```{marginfigure} ```` instead, then put the content in the chunk. See an example on the right about the first fundamental theorem of calculus.

```{marginfigure}
We know from _the first fundamental theorem of calculus_ that for $x$ in $[a, b]$:
$$\frac{d}{dx}\left( \int_{a}^{x} f(u)\,du\right)=f(x).$$
```

For the sake of portability between LaTeX and HTML, you should keep the margin content as simple as possible (syntax-wise) in the `marginefigure` blocks. You may use simple Markdown syntax like `**bold**` and `_italic_` text, but please refrain from using footnotes, citations, or block-level elements (e.g. blockquotes and lists) there.

Note: if you set `echo = FALSE` in your global chunk options, you will have to add `echo = TRUE` to the chunk to display a margin figure, for example ```` ```{marginfigure, echo = TRUE} ````.

## Full Width Figures

You can arrange for figures to span across the entire page by using the chunk option `fig.fullwidth = TRUE`.

```{r fig-fullwidth, fig.width = 10, fig.height = 2, fig.fullwidth = TRUE, fig.cap = "A full width figure.", warning=FALSE, message=FALSE, cache=TRUE}
ggplot(diamonds, aes(carat, price)) + geom_smooth() +
  facet_grid(~ cut)
```

 Other chunk options related to figures can still be used, such as `fig.width`, `fig.cap`, `out.width`, and so on. For full width figures, usually `fig.width` is large and `fig.height` is small. In the above example, the plot size is $10 \times 2$.

## Main Column Figures

Besides margin and full width figures, you can of course also include figures constrained to the main column. This is the default type of figures in the LaTeX/HTML output.

```{r fig-main, fig.cap = "A figure in the main column.", cache=TRUE}
ggplot(diamonds, aes(cut, price)) + geom_boxplot()
```

# Sidenotes

One of the most prominent and distinctive features of this style is the extensive use of sidenotes. There is a wide margin to provide ample room for sidenotes and small figures. Any use of a footnote will automatically be converted to a sidenote. ^[This is a sidenote that was entered using a footnote.] 

If you'd like to place ancillary information in the margin without the sidenote mark (the superscript number), you can use the `margin_note()` function from **tufte** in an inline R expression. `r margin_note("This is a margin note.  Notice that there is no number preceding the note.")` This function does not process the text with Pandoc, so Markdown syntax will not work here. If you need to write anything in Markdown syntax, please use the `marginfigure` block described previously.

# References

References can be displayed as margin notes for HTML output. For example, we can cite R here [@R-base]. To enable this feature, you must set `link-citations: yes` in the YAML metadata, and the version of `pandoc-citeproc` should be at least 0.7.2. You can always install your own version of Pandoc from http://pandoc.org/installing.html if the version is not sufficient. To check the version of `pandoc-citeproc` in your system, you may run this in R:

```{r eval=FALSE}
system2('pandoc-citeproc', '--version')
```

If your version of `pandoc-citeproc` is too low, or you did not set `link-citations: yes` in YAML, references in the HTML output will be placed at the end of the output document.

# Tables

You can use the `kable()` function from the **knitr** package to format tables that integrate well with the rest of the Tufte handout style. The table captions are placed in the margin like figures in the HTML output.

```{r}
knitr::kable(
  mtcars[1:6, 1:6], caption = 'A subset of mtcars.'
)
```

# Block Quotes

We know from the Markdown syntax that paragraphs that start with `>` are converted to block quotes. If you want to add a right-aligned footer for the quote, you may use the function `quote_footer()` from **tufte** in an inline R expression. Here is an example:

> "If it weren't for my lawyer, I'd still be in prison. It went a lot faster with two people digging."
>
> `r tufte::quote_footer('--- Joe Martin')`

Without using `quote_footer()`, it looks like this (the second line is just a normal paragraph):

> "Great people talk about ideas, average people talk about things, and small people talk about wine."
>
> --- Fran Lebowitz

# Responsiveness

The HTML page is responsive in the sense that when the page width is smaller than 760px, sidenotes and margin notes will be hidden by default. For sidenotes, you can click their numbers (the superscripts) to toggle their visibility. For margin notes, you may click the circled plus signs to toggle visibility.

# More Examples

The rest of this document consists of a few test cases to make sure everything still works well in slightly more complicated scenarios. First we generate two plots in one figure environment with the chunk option `fig.show = 'hold'`:

```{r fig-two-together, fig.cap="Two plots in one figure environment.", fig.show='hold', cache=TRUE, message=FALSE}
p <- ggplot(mtcars2, aes(hp, mpg, color = am)) +
  geom_point()
p
p + geom_smooth()
```

Then two plots in separate figure environments (the code is identical to the previous code chunk, but the chunk option is the default `fig.show = 'asis'` now):

```{r fig-two-separate, ref.label='fig-two-together', fig.cap=sprintf("Two plots in separate figure environments (the %s plot).", c("first", "second")), cache=TRUE, message=FALSE}
```

You may have noticed that the two figures have different captions, and that is because we used a character vector of length 2 for the chunk option `fig.cap` (something like `fig.cap = c('first plot', 'second plot')`).

Next we show multiple plots in margin figures. Similarly, two plots in the same figure environment in the margin:

```{r fig-margin-together, fig.margin=TRUE, fig.show='hold', fig.cap="Two plots in one figure environment in the margin.", fig.width=3.5, fig.height=2.5, cache=TRUE}
p
p + geom_smooth(method = 'lm')
```

Then two plots from the same code chunk placed in different figure environments:

```{r fig-margin-separate, fig.margin=TRUE, fig.cap=sprintf("Two plots in separate figure environments in the margin (the %s plot).", c("first", "second")), fig.width=3.5, fig.height=2.5, cache=TRUE}
knitr::kable(head(iris, 15))
p
knitr::kable(head(iris, 12))
p + geom_smooth(method = 'lm')
knitr::kable(head(iris, 5))
```

We blended some tables in the above code chunk only as _placeholders_ to make sure there is enough vertical space among the margin figures, otherwise they will be stacked tightly together. For a practical document, you should not insert too many margin figures consecutively and make the margin crowded. 

You do not have to assign captions to figures. We show three figures with no captions below in the margin, in the main column, and in full width, respectively.

```{r fig-nocap-margin, fig.margin=TRUE, fig.width=3.5, fig.height=2, cache=TRUE}
# a boxplot of weight vs transmission; this figure
# will be placed in the margin
ggplot(mtcars2, aes(am, wt)) + geom_boxplot() +
  coord_flip()
```
```{r fig-nocap-main, cache=TRUE}
# a figure in the main column
p <- ggplot(mtcars, aes(wt, hp)) + geom_point()
p
```
```{r fig-nocap-fullwidth, fig.fullwidth=TRUE, fig.width=10, fig.height=3, cache=TRUE}
# a fullwidth figure
p + geom_smooth(method = 'lm') + facet_grid(~ gear)
```

