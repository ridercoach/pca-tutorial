---
title: "PCA and Supervised Learning: A Simple Example"
author: "Eric N. Moyer"
date: "`r Sys.Date()`"
output:
  tufte::tufte_html: default
  tufte::tufte_handout:
    citation_package: natbib
    latex_engine: pdflatex
  tufte::tufte_book:
    citation_package: natbib
    latex_engine: pdflatex
bibliography: skeleton.bib
link-citations: yes
---

```{r setup, include=FALSE}
library(tufte)
library(tidyverse)
library(ggbiplot)
# invalidate cache when the tufte version changes
knitr::opts_chunk$set(tidy = FALSE, cache.extra = packageVersion('tufte'))
options(htmltools.dir.version = FALSE)
source("pca-utilities.R")
```

# Introduction

This exploration is inspired by Luke Hayden's excellent tutorial on DataCamp
about doing Principal Component Analysis in R,^[
[Link](https://www.datacamp.com/community/tutorials/pca-analysis-r) to the
article] in which he uses PCA and the `ggbiplot()` package to visually identify
groups in the data, which might be considered EDA or unsupervised learning.

In this article I want to use the same data and R function (`prcomp()`) to 
investigate how PCA can augment *supervised* learning; specifically, how the 
accuracy of a predictive model is affected by:

1. subsetting the original features based on insight gained from PCA;
1. replacing the original features with a much smaller set of transformed features.

# The Data

We will use the `mtcars` dataset built into R, but we will drop 
the two categorical features (eighth and ninth columns):

```{r data}
data <- mtcars[c(1:7, 10, 11)]
knitr::kable(
  data[1:4,], caption = 'The first few rows of the data'
)
```

This data set is very small, which is good for clarity in this example, 
but makes it impractical to split out a test set for model evaluation, so 
we will just use the RSE returned by `lm()` as a yardstick. I don't think 
this will completely invalidate our findings, but a good next step would 
be to try what we do here with a bigger set of data and proper model 
validation.

# Model for Evaluating the Effects of PCA

We will use a linear model, as implemented by the R function `lm()`, to 
predict **mpg** from the other features.

PCA requires the data it operates on to be centered and scaled, so, to be 
consistent, we will do the same thing to the non-target features when 
creating our baseline model:

```{r scaled_X}
scaled_X <- scale(data[,2:9], center = TRUE, scale = TRUE)
data_baseline <- as.data.frame(cbind(data[,1], scaled_X))
colnames(data_baseline)[1] <- "mpg"
knitr::kable(
  format(data_baseline[1:4,], digits = 3), 
  caption = 'Sample of data for baseline model'
)
```

Our baseline RSE, using all of the original features (which, with only nine
features, is what we would naturally do if we had no insight to the contrary)
is:

```{r mod_baseline}
sigma(lm(mpg ~ ., data_baseline))
```

# A Subtle Point

There is a subtle distinction when using PCA to augment supervised learning,
depending on whether we are trying to find the most important subset of our
original features, or whether we instead want to transform our original features
into composite features that better represent the variation in the data.

In either case, the contribution of PCA is to find the directions along which our
cloud of data points varies the most;^[If our data includes *n* features, then
each observation is a point in n-dimensional space, and all the observations
together might be envisioned as a "cloud" of such points.] it knows nothing
about which feature we are trying to predict.

If we are trying to subset our original features, then what we hope to gain 
from PCA is to determine which (if any) of the features (and the target) are 
more or less aligned with any of the principal components (directions of greatest 
variance.)  So in this case, we include the target in the data that gets 
transformed by PCA.

On the other hand, if we want to convert our original features into a new 
set of composite features,^[More accurately, transform our observations into 
a different coordinate system, in which the axes align with the principal 
components.] then we *cannot* include the target in the PCA, because we will 
"lose it" (it will get mixed into the principal components along with all the 
other features.)

In this article we will try out both approaches.

# Subsetting the Original Features

We pass the entire data set, including the target feature **mpg**, to the 
PCA function `prcomp()`, which returns a data structure containing a lot 
of interesting information.  We will look at three aspects of it.

## PCA Summary Information

```{r pca_subset_summary}
pca = prcomp(data, center = TRUE, scale = TRUE)
m <- format(summary(pca)$importance, digits = 1)
knitr::kable(
  as.data.frame(m), caption = 'PCA Summary.'
)
```

PCA transformed our nine original features into nine principal components, given
in order of decreasing importance.  PC1 represents the direction along which our
cloud of data points varies the most; the SD (first row of the table) of the
data along this direction is about 2.4, and 63% of the "signal" in the data
(second row) is seen along this axis. Notice also that the first two PCs
together account for 86% of the meaningful information in this data set (third
row.)

## Transformed Observations

Another thing returned by `prcomp()` is the coordinates of our data in the new 
system (in which the axes are along the directions of the PCs.)  The table below 
shows these new coordinates for the same few rows we have been looking at. *The 
"identities" of the observations within the data set have not changed;* they are 
simply being expressed as values for PC1, PC2, etc, instead of as values for 
**mpg**, **cyl**, etc.

```{r pca_subset_xformed_data}
knitr::kable(
  format(pca$x[1:4,], digits = 1), 
  caption = 'Sample of transformed data'
)
```

When our data is expressed in terms of the original physical features, 
it is difficult to represent visually; we can choose features for 2-D or 
3-D plots, but we have no way of knowing how close those plots come to telling 
the whole story.  Now, however, knowing that most of the variance in the 
data is captured in just two PCs, we *can* make a meaningful 2-D plot (see margin.) 
This plot is not necessary for what we are doing here, but it is interesting, 
and shows how PCA can reveal groups in the data.

```{r pca_subset_plot_pc1pc2, fig.margin = TRUE, fig.cap = "The data in terms of PC1 and PC2", fig.width=3.5, fig.height=3.5, cache=TRUE, message=FALSE}
xformed_data <- as.data.frame(pca$x)
ggplot(xformed_data) +
  geom_point(aes(PC1, PC2), color = "blue") +
  xlim(-4, 4) + ylim(-4, 4)
```

## Transformation Matrix

The information returned by `prcomp()` also includes a matrix that can be 
used and understood in several different ways (see table below.)

```{r pca_subset_matrix}
m <- as.data.frame(format(pca$rotation, digits = 1))
knitr::kable(
  m, caption = 'Transformation matrix'
)
```

Each column in the table gives the components (in terms of the original 
physical coordinate system) of a unit vector in the direction of one of the 
PCs.  (This matrix was used by `prcomp()` to produce the transformed data, and 
could be used to transform new observations in the same way;^[If you are not 
familiar with this process, there are lots of online sources that will help. If 
you have the time, the 14-part video series *Essence of Linear Algebra* on [3blue1brown](https://www.3blue1brown.com) is excellent.] see Appendix A for 
an example.) The greater the magnitude of a number in this matrix (regardless 
of sign,) the greater the alignment between the corresponding physical feature and 
principal component.

```{r pca_heatmap, fig.margin = TRUE, fig.cap = "Overall importance of original features in the dataset, according to PCA.", fig.width=3.5, fig.height=3.5, cache=TRUE, message=FALSE}
prop_var <- matrix(rep(summary(pca)$importance[2,], 9), 
                   nrow = 9, byrow = TRUE)
df <- cbind(rownames(pca$rotation), 
            data.frame(pca$rotation * prop_var))
colnames(df)[1] <- "feat"
df2 <- gather(df, colnames(df)[2:10], 
              key = "PC", value = "comp")
df2$feat <- factor(df2$feat, 
                   levels=(rev(rownames(pca$rotation))))
ggplot(df2, aes(x = PC, y = feat)) + 
  geom_tile(aes(fill = comp), colour = "white") +
  guides(fill = "none") +
  scale_fill_gradient2(low = "darkblue", 
                       mid = "white", high = "darkred")
```

The heatmap in the margin visually combines the information in the transformation
matrix and the PCA summary table.  In each column in the plot, bolder
colors indicate larger components along the original feature axes, and the
colors in each columnn are scaled by the "Proportion of Variance" in the PCA
Summary table.  So we see again that only the first 2 or 3 PCA
directions are meaningful, and within those we can see which original features
matter more.

# Feature Selection

A plot will make it easier to see alignment between the original features
and the first two principal components:^[The `ggbiplot` package will make a plot
similar to this, but it's more fun to do it ourselves.]

```{r pca_subset_biplot, fig.cap = "Original features as vectors in PC1-PC2 plane"}
df <- data.frame("feat" = rownames(pca$rotation),  
                 "PC1" = pca$rotation[,1], 
                 "PC2" = pca$rotation[,2])
ggplot(df) + geom_segment(aes(x = 0, y = 0, 
        xend = PC1, yend = PC2, color = feat)) +
  xlim(-0.5, 0.5) + ylim(-0.75, 0.75) + 
  labs(x = "PC1", y = "PC2")
```

The first thing we notice is that our target feature **mpg** and the **cyl** 
feature are closely aligned with the PC1 axis.  So let's try predicting **mpg** 
from *only* **cyl**:

```{r mod_cyl}
sigma(lm(mpg ~ cyl, data_baseline))
```

This yields an RSE of 3.21, which is worse than the baseline of 2.62, so let's 
try something else.  We see that **hp**, **disp**, and **wt** are also somewhat 
aligned with PC1, and, taken together, it appears that their misalignments might 
balance out, so:

```{r mod_cylplus}
sigma(lm(mpg ~ cyl + hp + disp + wt, data_baseline))
```

This yields a *better* fit than the baseline model that uses all the 
features.

So, while it bears repeating that we are using a very small data set and  
an overly-simplistic means of model evaluation, we have shown how PCA
can be used for feature selection.  Part of why this worked as well as
it did for this particular data set is that most of the data variation was
captured in the first two PCs.  This allowed us to plot the original features 
as 2-D vectors (without losing much information,) making the important 
features much easier to see.

# Transforming Original Features to PCA Features

Now we will look at another way PCA can augment supervised learning, which 
is by transforming the original features into the coordinate system of the 
principal components, and then using the most meaningful of those transformed 
features as input to the model.

As mentioned previously, with this approach we do not include the target feature 
in the PCA; here is the summary of the new PC analysis:

```{r pca_xfrom_summary}
pca = prcomp(data[c(2:9)], center = TRUE, scale = TRUE)
m <- format(summary(pca)$importance, digits = 1)
knitr::kable(
  as.data.frame(m), 
  caption = 'PCA Summary, target feature removed'
)
```

The numbers have changed a bit, but the first two principal components still 
represent 86% of the variation in the data.  And since the data returned by 
`prcomp()` includes the transformed features, we can very easily try out a 
model using PC1 and PC2:

```{r mod_pc1pc2}
df <- as.data.frame(cbind(data[,1], pca$x))
colnames(df)[1] <- "mpg"
sigma(lm(mpg ~ PC1 + PC2, df))
```

This is better than our baseline model, but actually not as good as the 
combination of original features **cyl**, **hp**, **disp**, and **wt**. 
Let's try adding one more PC:

```{r mod_pc1pc2pc3}
sigma(lm(mpg ~ PC1 + PC2 + PC3, df))
```

This is the best we have seen yet.  In general, we can keep adding PCs to the
model and the fit will always get better, because the PCs by definition are
along the directions of variance in the model. This is *not* the case with the
original features --- we may often get a better fit by using more of them (or
sometimes, as shown in the previous section, *fewer* of them,) but it is hard to
know intuitively which combination is best;^[There are various procedures for
feature selection that do not utilize PCA.] that was the whole point of how we
used PCA in the previous section.

# Conclusion

So it seems like using the transformed components is a more direct path to getting 
the best possible fit; why not just always do it this way?

If all we need is a black box that makes predictions as accurately as possible, 
there is no reason to *not* do it that way.  Transforming the features before 
feeding them into the model is not a big deal.

However, if what we want is *insight* --- if our motivation for modeling is to 
understand how to control real-world quantities in order to realize some practical 
benefit --- then the downside to using transformed features is that it can be 
difficult to understand how they map to the real world.

With the data set used in this article, that problem was not extreme; there 
were only a few original features, they were simple physical quantities, and 
the result that **mpg** could be predicted fairly well from only **cyl**, 
**hp**, **disp** and **wt** probably makes intuitive sense to many people. 
However, if we instead had hundreds of features that were more abstract, this 
might not be the case.

# Appendix A: Data Transformation Illustration

Transforming a point or vector from one coordinate system to another 
is almost always done by matrix-vector multiplication.^[You can learn about this from many online sources, including the [3blue1brown](https://www.3blue1brown.com) website 
mentioned previously.]

We saw above that when we give a data set to `prcomp()`, the information 
returned includes the coordinates of the data points in the new coordinate 
system (in which the axes are aligned with the principal components,) and 
the transformation matrix used to find those new coordinates.

Let's examine this process for the first row in our data (Mazda RX4.) The raw 
componenets of this observation in the original data are:

```{r}
knitr::kable(
  data[1,], caption = 'Original feature values, Mazda RX4'
)
```

The numbers in the above table are the coordinates of the Mazda RX4 data 
point in the original coordinate system (which simply means the terms in 
which the data was observed.) Now, recall that PCA requires data which has 
been centered and scaled, so let's see this data point in that form 
(note that these numbers are still in the original coordinate system):

```{r}
scaled_data <- as.data.frame(scale(data, 
                  center = TRUE, scale = TRUE))
knitr::kable(
  format(scaled_data[1,], digits = 3), 
  caption = 'Mazda RX4, centered and scaled'
)
```

Now let's run PCA on our raw data and view the first row of transformed 
coordinates:

```{r}
pca = prcomp(data, center = TRUE, scale = TRUE)
xformed_data <- as.data.frame(pca$x)
knitr::kable(
  format(xformed_data[1,], digits = 3), 
  caption = 'Mazda RX4, transformed by PCA'
)
```

The point of all this is verify that we can get the same numbers as in the 
above table by doing our own multiplication of the transformation matrix with 
the vector of coordinates in the original coordinate system, and you can 
see from the code below that we can:

```{r xform_check}
# vector in original coordinate system
v_old <- t(scaled_data[1,])
# transformation matrix, transposed as nec
A <- t(pca$rotation)
# vector in PCA coordinate system
v_new <- A %*% v_old

knitr::kable(
  format(t(v_new), digits = 1), 
  caption = "Mazda RX4, transformed by us"
)
```

Our transformation matches what was done by `prcomp()`. If we wanted to 
make predictions for new observations using a model that takes PCA features 
as input, we would have to first transform the new observations similar to 
the above.




